{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95821522",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 0\n",
    "num_workers = 4\n",
    "seed = 42\n",
    "target_name = 'p'\n",
    "exclude_pairs = [[\"DMN\",\"DMN\"]]\n",
    "patch_size_ratio = 1\n",
    "repeat_num = 1\n",
    "\n",
    "model_path = \"/users/hjd/HJW_take_over/SNUH_hjd/best_models_for_manuscript/pretrain_model/Hsp:[0.95,0.3,0.3]_Maxb:[0.01, 0.05, 0.05]_Betalr:[0.0001, 0.001, 0.001]_LR:[5e-05]_Act:[elu]_Opt:[nag]_DO:[0.9,0.9,0.0]_lambda:[0.01]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5716958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'BSPL'\n",
    "data_path_dict={\n",
    "    'BSPL':{\n",
    "        \"Parcel\":\"/data2/alumni/hjw/data/ABCD/Parcels/Parcels.xlsx\",\n",
    "        \"Gordon\":\"/data2/alumni/hjw/data/ABCD/npz_files/Gordon_network_labels.npz\",\n",
    "        \"ABCD_rsfc\":\"/data2/alumni/hjw/data/ABCD/npz_files/rsfc_p_site_scanner_si_ge.npz\",\n",
    "        \"ABCD_CFA\":\"/data4/SNU/data/ABCD_CFA_5factor.npz\",\n",
    "#         \"SNUH_rsfc\":f\"/data4/SNU/data/snuh_fc_temp_hjd/RSFC_Smoothing[{smoothing}]_GSR[{gsr}]_Censor[{censor}].npz\",\n",
    "        \"SNUH_demo\":\"/users/hjd/Revision/demo_for_rsfc_df.csv\",\n",
    "        \"SNUH_CFA\":\"\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47b5061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.24.1', '1.8.1+cu102')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn, torch\n",
    "sklearn.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8c5517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold: [0 1 2 3 4]\n",
      "Selected Fold: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "temp_sel_idx = 0\n",
    "n_cv = 5\n",
    "\n",
    "import numpy as np\n",
    "sel_cv_idx = 0\n",
    "jump_val = n_cv\n",
    "outer_cv_part = np.arange(sel_cv_idx * jump_val, sel_cv_idx * jump_val + jump_val)\n",
    "print(\"Total Fold: {}\".format(outer_cv_part))\n",
    "\n",
    "select_fold = outer_cv_part\n",
    "print(\"Selected Fold: {}\".format(select_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32af4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "act_func_name = \"elu\"\n",
    "optimizer_name = \"nag\"\n",
    "\n",
    "h1_cand = [1024]\n",
    "h2_cand = [1024]\n",
    "\n",
    "dropout_h1_cand = [0.3]\n",
    "dropout_h2_cand = [0.3]\n",
    "batch_size_cand = [4]\n",
    "lr_cand = [4e-05]\n",
    "lr_patience_cand = [5]\n",
    "lr_factor_cand = [0.5]\n",
    "epochs_cand = [150]\n",
    "\n",
    "hsp_h1_cand = [0.95]\n",
    "hsp_h2_cand = [0.3]\n",
    "\n",
    "l2_param_cand = [2e-01]\n",
    "\n",
    "pretrain_cand = [True]\n",
    "trainable_ext_cand = [False]\n",
    "trainable_prd_cand = [True]\n",
    "\n",
    "param_cand = {\n",
    "    \"h1\": h1_cand, \"h2\": h2_cand,  \n",
    "    \"dropout_h1\": dropout_h1_cand, \"dropout_h2\": dropout_h2_cand,\n",
    "    \"batch_size\": batch_size_cand, \"lr\": lr_cand, \"epochs\": epochs_cand, \n",
    "    \"lr_patience\": lr_patience_cand, \"lr_factor\": lr_factor_cand,\n",
    "    \"hsp_h1\": hsp_h1_cand, \"hsp_h2\": hsp_h2_cand, \"l2_param\": l2_param_cand,\n",
    "    \"pretrain\": pretrain_cand, \n",
    "    \"freeze_ext\": trainable_ext_cand, \"freeze_prd\": trainable_prd_cand\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b784cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from decimal import Decimal\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from pytz import timezone\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingWarmRestarts\n",
    "from torch.optim.swa_utils import SWALR, AveragedModel\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb0d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(seed)\n",
    "\n",
    "nowtime = dt.now(timezone(\"Asia/Seoul\")); year = str(nowtime.year)[2:]\n",
    "month = '0{}'.format(nowtime.month) if nowtime.month < 10 else str(nowtime.month)\n",
    "day = '0{}'.format(nowtime.day) if nowtime.day < 10 else str(nowtime.day)\n",
    "hour = '0{}'.format(nowtime.hour) if nowtime.hour < 10 else str(nowtime.hour)\n",
    "minute = '0{}'.format(nowtime.minute) if nowtime.minute < 10 else str(nowtime.minute)\n",
    "sec = '0{}'.format(nowtime.second) if nowtime.second < 10 else str(nowtime.second)\n",
    "msec = str(nowtime.microsecond)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d8fd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_fn_pairs(input_rsfc,exclude_pairs,patch_ratio):\n",
    "    \n",
    "    tot_rois = 352\n",
    "\n",
    "    parcels = pd.read_excel(\"/data2/alumni/hjw/data/ABCD/Parcels/Parcels.xlsx\", engine=\"openpyxl\")\n",
    "    networks = list(parcels[\"Community\"]) + 19 * [\"Subcortex\"]\n",
    "    networks_df = pd.DataFrame(networks, columns=[\"network\"])\n",
    "    networks_df[networks_df[\"network\"] == \"Auditory\"] = \"AUD\"\n",
    "    networks_df[networks_df[\"network\"] == \"Visual\"] = \"VIS\"\n",
    "    networks_df[networks_df[\"network\"] == \"VentralAttn\"] = \"VAN\"\n",
    "    networks_df[networks_df[\"network\"] == \"Subcortex\"] = \"SCN\"\n",
    "    networks_df[networks_df[\"network\"] == \"Salience\"] = \"SAL\"\n",
    "    networks_df[networks_df[\"network\"] == \"SMmouth\"] = \"SMM\"\n",
    "    networks_df[networks_df[\"network\"] == \"SMhand\"] = \"SMH\"\n",
    "    networks_df[networks_df[\"network\"] == \"RetrosplenialTemporal\"] = \"RSP\"\n",
    "    networks_df[networks_df[\"network\"] == \"None\"] = \"NONE\"\n",
    "    networks_df[networks_df[\"network\"] == \"FrontoParietal\"] = \"FPN\"\n",
    "    networks_df[networks_df[\"network\"] == \"DorsalAttn\"] = \"DAN\"\n",
    "    networks_df[networks_df[\"network\"] == \"Default\"] = \"DMN\"\n",
    "    networks_df[networks_df[\"network\"] == \"CinguloParietal\"] = \"CPAR\"\n",
    "    networks_df[networks_df[\"network\"] == \"CinguloOperc\"] = \"CON\"\n",
    "\n",
    "    orig_networks = networks_df['network'].tolist()\n",
    "    networks_label = sorted(np.unique(networks_df['network'].values))\n",
    "    new_networks_label = networks_label.copy()\n",
    "    new_networks_label.remove(\"NONE\")\n",
    "    new_networks_label.append(\"NONE\")\n",
    "\n",
    "    new_orig_network_order = {\n",
    "        key:value for (key, value) in \n",
    "        zip(new_networks_label, (np.arange(len(new_networks_label))))\n",
    "    }\n",
    "\n",
    "\n",
    "    sorted_networks = pd.DataFrame(index=orig_networks).sort_index(key=lambda x: x.map(new_orig_network_order), axis=0).index.tolist()\n",
    "    sorted_networks_df = pd.DataFrame(np.unique(sorted_networks, return_index=True)).T\n",
    "    sorted_networks_df.columns = [\"networks\", \"n\"]\n",
    "    sorted_networks_df = sorted_networks_df.sort_values(\n",
    "        by=\"networks\", key=lambda x: x.map(new_orig_network_order)\n",
    "    )\n",
    "\n",
    "    for idx,vec in enumerate(input_rsfc):\n",
    "        wfm = np.zeros((tot_rois, tot_rois))\n",
    "        iu_non_di_idx = np.mask_indices(tot_rois, np.triu, 1)\n",
    "        wfm[iu_non_di_idx] = vec\n",
    "        il_idx = np.tril_indices(tot_rois, -1)\n",
    "        wfm[il_idx] = wfm.T[il_idx]\n",
    "        wfm_df = pd.DataFrame(wfm, index=orig_networks, columns=orig_networks)\n",
    "\n",
    "        for pairs in exclude_pairs:\n",
    "            flat_fn = np.array(wfm_df.loc[pairs[0],pairs[1]]).ravel()\n",
    "            fn_size = len(flat_fn)\n",
    "            patch_size = int(patch_ratio*fn_size)\n",
    "            patch_idx = np.random.choice(np.arange(fn_size),patch_size,replace=False)\n",
    "            flat_fn[patch_idx] = 0\n",
    "            recon_fn = flat_fn.reshape(wfm_df.loc[pairs[0],pairs[1]].shape)\n",
    "            wfm_df.loc[pairs[0],pairs[1]] = recon_fn\n",
    "            wfm_df.loc[pairs[1],pairs[0]]= recon_fn.T\n",
    "\n",
    "        wfm_df_vis = wfm_df.copy()\n",
    "        wfm_df_vis = wfm_df_vis.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "        wfm_df_vis = wfm_df_vis.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "        \n",
    "        if idx==0:\n",
    "            excluded_rsfc = wfm[iu_non_di_idx]\n",
    "            excluded_rsfc_vis = np.expand_dims(np.array(wfm_df_vis),axis=0)\n",
    "        else:\n",
    "            excluded_rsfc = np.vstack([excluded_rsfc,wfm[iu_non_di_idx]])\n",
    "            excluded_rsfc_vis = np.vstack([excluded_rsfc_vis,np.expand_dims(np.array(wfm_df_vis),axis=0)])\n",
    "    if len(exclude_pairs)>0:\n",
    "        return excluded_rsfc, excluded_rsfc_vis, patch_idx\n",
    "    else:\n",
    "        return excluded_rsfc, excluded_rsfc_vis, 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fc8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_factor_idx = 0\n",
    "\n",
    "data_path = data_path_dict[location]['ABCD_rsfc']\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "X_original = data[\"X\"]\n",
    "if exclude_pairs ==[]:\n",
    "    X = X_original, X_original\n",
    "else:\n",
    "    X, X_vis,_ = exclude_fn_pairs(X_original,exclude_pairs,1)\n",
    "\n",
    "targets_all = np.load(data_path_dict[location]['ABCD_CFA'],allow_pickle=True) \n",
    "target_fs = targets_all[target_name]\n",
    "target_scn = targets_all['scn']\n",
    "y = target_fs.reshape(-1,1)\n",
    "print(X.max(),X.min())\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ffc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exclude_pairs !=[]:\n",
    "    wfm_df_vis = np.mean(X_vis,axis=0)\n",
    "\n",
    "    tot_rois = 352\n",
    "\n",
    "    parcels = pd.read_excel(\"/data2/alumni/hjw/data/ABCD/Parcels/Parcels.xlsx\", engine=\"openpyxl\")\n",
    "    networks = list(parcels[\"Community\"]) + 19 * [\"Subcortex\"]\n",
    "    networks_df = pd.DataFrame(networks, columns=[\"network\"])\n",
    "    networks_df[networks_df[\"network\"] == \"Auditory\"] = \"AUD\"\n",
    "    networks_df[networks_df[\"network\"] == \"Visual\"] = \"VIS\"\n",
    "    networks_df[networks_df[\"network\"] == \"VentralAttn\"] = \"VAN\"\n",
    "    networks_df[networks_df[\"network\"] == \"Subcortex\"] = \"SCN\"\n",
    "    networks_df[networks_df[\"network\"] == \"Salience\"] = \"SAL\"\n",
    "    networks_df[networks_df[\"network\"] == \"SMmouth\"] = \"SMM\"\n",
    "    networks_df[networks_df[\"network\"] == \"SMhand\"] = \"SMH\"\n",
    "    networks_df[networks_df[\"network\"] == \"RetrosplenialTemporal\"] = \"RSP\"\n",
    "    networks_df[networks_df[\"network\"] == \"None\"] = \"NONE\"\n",
    "    networks_df[networks_df[\"network\"] == \"FrontoParietal\"] = \"FPN\"\n",
    "    networks_df[networks_df[\"network\"] == \"DorsalAttn\"] = \"DAN\"\n",
    "    networks_df[networks_df[\"network\"] == \"Default\"] = \"DMN\"\n",
    "    networks_df[networks_df[\"network\"] == \"CinguloParietal\"] = \"CPAR\"\n",
    "    networks_df[networks_df[\"network\"] == \"CinguloOperc\"] = \"CON\"\n",
    "\n",
    "    orig_networks = networks_df['network'].tolist()\n",
    "    networks_label = sorted(np.unique(networks_df['network'].values))\n",
    "    new_networks_label = networks_label.copy()\n",
    "    new_networks_label.remove(\"NONE\")\n",
    "    new_networks_label.append(\"NONE\")\n",
    "\n",
    "\n",
    "    new_orig_network_order = {\n",
    "        key:value for (key, value) in \n",
    "        zip(new_networks_label, (np.arange(len(new_networks_label))))\n",
    "    }\n",
    "\n",
    "\n",
    "    sorted_networks = pd.DataFrame(index=orig_networks).sort_index(key=lambda x: x.map(new_orig_network_order), axis=0).index.tolist()\n",
    "    sorted_networks_df = pd.DataFrame(np.unique(sorted_networks, return_index=True)).T\n",
    "    sorted_networks_df.columns = [\"networks\", \"n\"]\n",
    "    sorted_networks_df = sorted_networks_df.sort_values(\n",
    "        by=\"networks\", key=lambda x: x.map(new_orig_network_order)\n",
    "    )\n",
    "\n",
    "\n",
    "    start_network_idx = np.array(sorted_networks_df.n)\n",
    "    next_network_idx = np.hstack((start_network_idx[1:], 352))\n",
    "\n",
    "    network_mid_idx = np.array((start_network_idx + next_network_idx) / 2, dtype=int)\n",
    "\n",
    "    sns.set(style=\"white\", font_scale=3)\n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    cbar_kws = dict(use_gridspec=False, shrink=0.85, location=\"right\")\n",
    "    threshold = 1\n",
    "    sns.heatmap(\n",
    "        wfm_df_vis, square=True, cmap=\"RdBu_r\", center=0, \n",
    "        vmax=threshold, vmin=-threshold , \n",
    "        mask=np.triu(np.ones(wfm_df_vis.shape), 1).astype(bool),\n",
    "        ax=ax, cbar_kws=cbar_kws\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(network_mid_idx)\n",
    "    ax.set_yticks(network_mid_idx)\n",
    "\n",
    "    ax.set_xticklabels(new_networks_label, rotation=90, fontsize=45, ha=\"center\")\n",
    "    ax.set_yticklabels(new_networks_label, rotation='horizontal', fontsize=45)\n",
    "\n",
    "    for network_pos in next_network_idx:\n",
    "        plt.axvline(network_pos, linewidth=1.5, color=\"black\", ymin=0, ymax=network_pos)\n",
    "        plt.axhline(network_pos, linewidth=1.5, color=\"black\", xmin=0, xmax=network_pos)\n",
    "\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(\"$RSFC$\", fontsize=60, labelpad=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed)\n",
    "\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "\n",
    "outer_n_splits = n_cv\n",
    "\n",
    "outer_train_folds_idx = []\n",
    "outer_test_folds_idx = []\n",
    "\n",
    "outer_skf = ShuffleSplit(\n",
    "    n_splits=outer_n_splits, test_size=0.20, random_state=seed)\n",
    "\n",
    "for n_outer, (outer_train_idx, outer_test_idx) in enumerate(outer_skf.split(X, y)):\n",
    "    outer_train_folds_idx.append(outer_train_idx)\n",
    "    outer_test_folds_idx.append(outer_test_idx)\n",
    "\n",
    "len(outer_train_folds_idx),len(outer_train_folds_idx[0]), len(outer_test_folds_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set order of network label \n",
    "new_orig_network_idx_order = [\n",
    "    'AUD', 'DAN', 'CPAR', 'NONE', \n",
    "    'SAL', 'VIS', 'RSP', 'DMN',\n",
    "    'SMM', 'CON', 'SCN', \n",
    "    'SMH', 'VAN', 'FPN'\n",
    "]\n",
    "\n",
    "sorted_order = sorted(new_orig_network_idx_order)\n",
    "sorted_order.remove(\"NONE\")\n",
    "sorted_order.append(\"NONE\")\n",
    "new_orig_network_idx_order = sorted_order\n",
    "print(new_orig_network_idx_order)\n",
    "\n",
    "new_orig_network_order = {\n",
    "    key:value for (key, value) in \n",
    "    zip(new_orig_network_idx_order, (np.arange(len(new_orig_network_idx_order))))\n",
    "}\n",
    "\n",
    "def make_wfm(vec):\n",
    "    wfm = np.zeros((tot_rois, tot_rois))\n",
    "    iu_non_di_idx = np.mask_indices(tot_rois, np.triu, 1)\n",
    "    wfm[iu_non_di_idx] = vec\n",
    "    il_idx = np.tril_indices(tot_rois, -1)\n",
    "    wfm[il_idx] = wfm.T[il_idx]\n",
    "    wfm_df = pd.DataFrame(wfm, index=orig_networks, columns=orig_networks)\n",
    "    wfm_df = wfm_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "    wfm_df = wfm_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "    \n",
    "    return wfm_df\n",
    "\n",
    "\n",
    "#input으로 model, epoch받아서 wfm 저장시키기\n",
    "\n",
    "def visualize_wfm(trained_model,epoch,epoch_gap = 10,threshold=False,mode = 'sum'):\n",
    "    \n",
    "    if epoch%epoch_gap !=0:\n",
    "        return\n",
    "    \n",
    "    wfm_save_dir = outer_save_dir+'/WFM/'\n",
    "    if not os.path.isdir(wfm_save_dir):\n",
    "        os.mkdir(wfm_save_dir)\n",
    "    \n",
    "    w_ext = []\n",
    "    w_reg = []\n",
    "    w_dsc = []\n",
    "\n",
    "    ext_hidden = trained_model.ext_1.weight.shape[0]\n",
    "    prd_hidden = trained_model.prd_1.weight.shape[0]\n",
    "\n",
    "    w = []\n",
    "\n",
    "    for name,params in trained_model.named_parameters():\n",
    "        if \"weight\" in name and \"bn\" not in name:\n",
    "            if \"ext\" in name or \"prd\" in name:\n",
    "                w.append(params)\n",
    "\n",
    "    w_ext_1 = w[0].detach().cpu().numpy().T\n",
    "    w_reg_1 = w[1].detach().cpu().numpy().T\n",
    "    w_reg_2 = w[2].detach().cpu().numpy().T\n",
    "\n",
    "    temp_w_ext = w_ext_1\n",
    "    temp_w_reg = np.matmul(np.matmul(temp_w_ext, w_reg_1), w_reg_2)\n",
    "\n",
    "    w_ext.append(temp_w_ext) #ext x pred1 (352x1024)\n",
    "    w_reg.append(temp_w_reg) #ext x pred1 x pred2 (352x1)\n",
    "\n",
    "#     np.savez(wfm_save_dir+\"/wfm_reg_epoch{}\".format(epoch), X=w_reg)\n",
    "\n",
    "    wfm_df = make_wfm(stats.zscore(w_reg[0].reshape(-1,)))\n",
    "\n",
    "    cols = rows = wfm_df.columns.values\n",
    "#     new_orig_network_idx_order = np.unique(cols)\n",
    "    avg_df_1 = pd.DataFrame(columns=cols, index=rows)\n",
    "    avg_df_2 = pd.DataFrame(columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "    avg_df_3 = pd.DataFrame(columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "\n",
    "    for i, temp_row in enumerate(new_orig_network_idx_order):\n",
    "        for j, temp_col in enumerate(new_orig_network_idx_order):\n",
    "            temp_row_ids = np.where(rows == temp_row)[0]\n",
    "            temp_col_ids = np.where(cols == temp_col)[0]\n",
    "            temp_mat = wfm_df.iloc[temp_row_ids, temp_col_ids]\n",
    "            temp_vec = temp_mat.values.ravel()\n",
    "            n_temp_vec = np.sum(np.absolute(temp_mat.values) > 1.96)\n",
    "            if mode == 'sum':\n",
    "                sum_val = temp_vec.sum()\n",
    "                n_val = n_temp_vec\n",
    "            elif mode == 'avg':\n",
    "                if temp_row == temp_col:\n",
    "                    sum_val = temp_vec.sum() / (len(temp_row_ids) * (len(temp_row_ids) - 1))\n",
    "                    n_val = n_temp_vec / (len(temp_row_ids) * (len(temp_row_ids) - 1))\n",
    "                else:\n",
    "                    sum_val = temp_vec.sum() / (len(temp_row_ids) * (len(temp_col_ids)))\n",
    "                    n_val = n_temp_vec / (len(temp_row_ids) * (len(temp_col_ids)))\n",
    "            avg_df_1.iloc[temp_row_ids, temp_col_ids] = sum_val\n",
    "            avg_df_2.loc[temp_row, temp_col] = sum_val\n",
    "            avg_df_3.loc[temp_row, temp_col] = n_val\n",
    "\n",
    "    if threshold==False:\n",
    "        avg_mat = np.array(avg_df_2.values, dtype=float)\n",
    "        avg_mat = (avg_mat - avg_mat.mean()) / avg_mat.std()\n",
    "        avg_mat_df = pd.DataFrame(avg_mat, columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "        thr_zero = 0\n",
    "        avg_mat_df[(avg_mat_df < thr_zero) & (avg_mat_df > -thr_zero)] = 0\n",
    "\n",
    "        sns.set(style=\"white\", font_scale=4.5)\n",
    "        fig, ax = plt.subplots(figsize=(32, 32))\n",
    "        threshold = np.abs(avg_mat_df.values).max()\n",
    "        threshold = 5.5\n",
    "        cbar_kws = dict(\n",
    "            use_gridspec=False, shrink=0.85, location=\"right\", label=\"Correlation ($r$)\")\n",
    "        sns.heatmap(\n",
    "            avg_mat_df, square=True, cmap=\"vlag\", ax=ax, \n",
    "            mask=np.triu(np.ones(avg_mat_df.shape), 1).astype(bool),\n",
    "            vmax=threshold, vmin=-threshold, \n",
    "            linewidths=5, \n",
    "            cbar=True, cbar_kws=cbar_kws,\n",
    "            fmt=\".2f\", annot=True, annot_kws={\"fontsize\": 32}\n",
    "        )\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_label(\"$WF$\", fontsize=60, labelpad=50)\n",
    "        ax.set_xticklabels(new_orig_network_order, rotation=90, fontsize=60)\n",
    "        ax.set_yticklabels(new_orig_network_order, rotation=0, fontsize=60)\n",
    "        plt.title(f\"Epoch {epoch}/{epochs}\")\n",
    "        plt.savefig(wfm_save_dir+f\"/{mode}_WFM_epoch{epoch}.jpg\")\n",
    "        plt.close(fig)\n",
    "    #     plt.show()\n",
    "\n",
    "    if threshold==True:\n",
    "        avg_mat = np.array(avg_df_2.values, dtype=float)\n",
    "        avg_mat = (avg_mat - avg_mat.mean()) / avg_mat.std()\n",
    "        avg_mat_df = pd.DataFrame(avg_mat, columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "\n",
    "        annot = np.vectorize(lambda x: '' if np.absolute(x) < 3.091 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "        # annot = np.vectorize(lambda x: '' if np.absolute(x) < 2.58 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "        # annot = np.vectorize(lambda x: '' if np.absolute(x) < 1.96 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "\n",
    "        thr_zero = 0\n",
    "        avg_mat_df[(avg_mat_df < thr_zero) & (avg_mat_df > -thr_zero)] = 0\n",
    "\n",
    "        sns.set(style=\"white\", font_scale=5)\n",
    "        plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "        plt.rcParams['mathtext.it'] = 'Arial:italic'\n",
    "        plt.rcParams['mathtext.rm'] = 'Arial'\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(32, 32))\n",
    "        threshold = np.abs(avg_mat_df.values).max()\n",
    "        threshold = 7\n",
    "        cbar_kws = dict(\n",
    "            use_gridspec=False, shrink=0.85, location=\"right\", label=\"Correlation ($r$)\")\n",
    "        sns.heatmap(\n",
    "            avg_mat_df, square=True, cmap=\"vlag\",\n",
    "            ax=ax,\n",
    "            mask=np.triu(np.ones(avg_mat_df.shape), 1).astype(bool),\n",
    "            vmax=threshold, vmin=-threshold, \n",
    "            linewidths=5, \n",
    "            cbar=True, cbar_kws=cbar_kws,\n",
    "            fmt=\"\", annot=annot, annot_kws={\"fontsize\": 36}\n",
    "        )\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_label(\"$WF$\", fontsize=60, labelpad=50)\n",
    "        ax.set_xticklabels(new_orig_network_order, rotation=90, fontsize=60)\n",
    "        ax.set_yticklabels(new_orig_network_order, rotation=0, fontsize=60)\n",
    "        plt.title(f\"Epoch {epoch}/{epochs}\")\n",
    "        plt.savefig(wfm_save_dir+f\"/{mode}_WFM_epoch{epoch}.jpg\")\n",
    "        plt.close(fig)\n",
    "    #     plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "def plot_prediction_result(epoch,pearsonr,valid_prediction,valid_true):\n",
    "    \n",
    "    pred_save_dir = outer_save_dir+'/Prediction/'\n",
    "    if not os.path.isdir(pred_save_dir):\n",
    "        os.mkdir(pred_save_dir)\n",
    "    \n",
    "    sns.set(style=\"darkgrid\", font_scale=2)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    fig = sns.scatterplot(valid_true, valid_prediction,s=100)\n",
    "    plt.title(f\"Epoch {epoch}/{epochs}, Pearson's r : %.3f\" % pearsonr)\n",
    "    plt.xlabel(\"True $p$-factor\")\n",
    "    plt.ylabel(\"Predicted $p$-factor\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(np.min(valid_prediction)-0.0002,np.max(valid_prediction)+0.0002)\n",
    "    plt.savefig(pred_save_dir+f\"/Prediction_epoch{epoch}.png\")\n",
    "    plt.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"max\"\n",
    "min_lr = 1e-08\n",
    "lr_alpha = -1.5\n",
    "lr_beta = 1.7\n",
    "\n",
    "momentum = 0.90\n",
    "l1_param = 0\n",
    "early_stopping_patience = 150\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "n_classes = 1\n",
    "output_dim = n_classes\n",
    "\n",
    "wsc_flag = [0, 0]\n",
    "beta_lr = [5e-03, 1e-02]\n",
    "max_beta = [5e-02, 1e-01]\n",
    "n_wsc = wsc_flag.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "class TrainDataset(Dataset): \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_train = torch.from_numpy(self.X_train[idx]).type(torch.FloatTensor)\n",
    "        y_train = torch.from_numpy(self.y_train[idx]).type(torch.FloatTensor)\n",
    "\n",
    "        return X_train, y_train\n",
    "    \n",
    "# Test dataset\n",
    "class TestDataset(Dataset): \n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_test)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_test = torch.from_numpy(self.X_test[idx]).type(torch.FloatTensor)\n",
    "        y_test = torch.from_numpy(self.y_test[idx]).type(torch.FloatTensor)\n",
    "        \n",
    "        return X_test, y_test\n",
    "    \n",
    "    \n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, h1, h2, dropout_h1, dropout_h2, act_func_name):\n",
    "        super(DNN, self).__init__()\n",
    "        self.ext_1 = nn.Linear(input_dim, h1)\n",
    "        self.ext_bn_1 = nn.BatchNorm1d(h1)\n",
    "        \n",
    "        self.prd_1 = nn.Linear(h1, h2)\n",
    "        self.prd_bn_1 = nn.BatchNorm1d(h2)\n",
    "        self.prd_2 = nn.Linear(h2, output_dim)\n",
    "        \n",
    "        self.dropout_h1 = nn.Dropout(p=dropout_h1)\n",
    "        self.dropout_h2 = nn.Dropout(p=dropout_h2)\n",
    "        \n",
    "        self.act_func = get_act_func(act_func_name)\n",
    "        self.weights_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ext_1(x)\n",
    "        x = self.ext_bn_1(x)\n",
    "        x = self.act_func(x)\n",
    "        x = self.dropout_h1(x)\n",
    "        \n",
    "        x = self.prd_1(x)\n",
    "        x = self.prd_bn_1(x)\n",
    "        x = self.act_func(x)\n",
    "        x = self.dropout_h2(x)\n",
    "        out = self.prd_2(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                nn.init.normal_(m.bias, std=0.01)\n",
    "    \n",
    "def get_optimizer(model, opt_name, learning_rate=None, l2_param=None):\n",
    "    lower_opt_name = opt_name.lower()\n",
    "    if lower_opt_name == 'momentum':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'nag':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param, nesterov=True)\n",
    "    elif lower_opt_name == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'sparseadam':\n",
    "        return optim.SparseAdam(model.parameters(), lr=learning_rate,\n",
    "                       betas=(0.9, 0.999), eps=1e-08, maximize=False)\n",
    "    elif lower_opt_name == 'radam':\n",
    "        return optim.RAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999),\n",
    "                           eps=1e-08, weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'nadam':\n",
    "        return optim.NAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08,\n",
    "                  weight_decay=l2_param, momentum_decay=0.004)\n",
    "    elif lower_opt_name == 'adamax':\n",
    "        return optim.Adamax(model.parameters(), lr=learning_rate, betas=(0.9, 0.999),\n",
    "                            eps=1e-08, weight_decay=l2_param)\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for optimizer type\")\n",
    "        \n",
    "def get_act_func(act_func_name):\n",
    "    act_func_name = act_func_name.lower()\n",
    "    if act_func_name == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act_func_name == 'prelu':\n",
    "        return nn.PReLU()\n",
    "    elif act_func_name == 'elu':\n",
    "        return nn.ELU()\n",
    "    elif act_func_name == 'silu':\n",
    "        return nn.SiLU()\n",
    "    elif act_func_name == 'leakyrelu':\n",
    "        return nn.LeakyReLU()\n",
    "    elif act_func_name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act_func_name == 'selu':\n",
    "        return nn.SELU()\n",
    "    elif act_func_name == 'gelu':\n",
    "        return nn.GELU()\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for activation function type\")\n",
    "        \n",
    "def init_hsp(n_wsc, epochs):\n",
    "    hsp_val = torch.zeros(n_wsc)\n",
    "    beta_val = torch.clone(hsp_val)\n",
    "    hsp_list = torch.zeros((n_wsc, epochs))\n",
    "    beta_list = torch.zeros((n_wsc, epochs))\n",
    "    \n",
    "    return hsp_val, beta_val, hsp_list, beta_list\n",
    "\n",
    "# Weight sparsity control with Hoyer's sparsness (Layer wise)\n",
    "def calc_hsp(w, beta, max_beta, beta_lr, tg_hsp):\n",
    "    \n",
    "    # Get value of weight\n",
    "    [dim, n_nodes] = w.shape\n",
    "    num_elements = dim * n_nodes\n",
    "    norm_ratio = torch.norm(w.detach(), 1) / torch.norm(w.detach(), 2)\n",
    "\n",
    "    # Calculate hoyer's sparsity level\n",
    "    num = math.sqrt(num_elements) - norm_ratio\n",
    "    den = math.sqrt(num_elements) - 1\n",
    "    hsp = torch.tensor(num / den).to(device)\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta.clone() + beta_lr * torch.sign(torch.tensor(tg_hsp).to(device) - hsp)\n",
    "    \n",
    "    # Trim value\n",
    "    beta = 0 if beta < 0 else beta\n",
    "    beta = max_beta if beta > max_beta else beta\n",
    "\n",
    "    return [hsp, beta]\n",
    "\n",
    "def calc_l1(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
    "    l1_reg = None\n",
    "    layer_idx = 0\n",
    "    wsc_idx = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and \"bn\" not in name:\n",
    "            if \"ext\" in name or \"prd_1\" in name:\n",
    "                temp_w = param\n",
    "                \n",
    "                if wsc_flag[layer_idx] != 0:\n",
    "                    hsp_val[wsc_idx], beta_val[wsc_idx] = calc_hsp(\n",
    "                        temp_w, beta_val[wsc_idx], max_beta[wsc_idx], \n",
    "                        beta_lr[wsc_idx], tg_hsp[wsc_idx]\n",
    "                    )\n",
    "                    hsp_list[wsc_idx, epoch - 1] = hsp_val[wsc_idx]\n",
    "                    beta_list[wsc_idx, epoch - 1] = beta_val[wsc_idx]\n",
    "                    layer_reg = torch.norm(temp_w, 1) * beta_val[wsc_idx].clone()\n",
    "                    wsc_idx += 1\n",
    "                else:\n",
    "                    layer_reg = torch.norm(temp_w, 1) * l1_param\n",
    "\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = layer_reg\n",
    "                else:\n",
    "                    l1_reg = l1_reg + layer_reg\n",
    "                layer_idx += 1\n",
    "        \n",
    "    return l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total = 0\n",
    "    y_test_true = []\n",
    "    y_test_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(test_loader):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            pred, target = model(input), target.view(-1, 1)\n",
    "            running_loss = criterion(pred, target)\n",
    "            test_loss += running_loss.item()\n",
    "            total += pred.size(0)\n",
    "\n",
    "            true_batch = torch.flatten(target).detach().cpu().numpy()\n",
    "            pred_batch = torch.flatten(pred).detach().cpu().numpy()\n",
    "            y_test_true.extend(true_batch)\n",
    "            y_test_pred.extend(pred_batch)\n",
    "\n",
    "        test_acc,test_p = stats.pearsonr(y_test_true, y_test_pred)\n",
    "        test_loss = test_loss / (batch_idx+1)\n",
    "        test_mae = mean_absolute_error(y_test_pred,y_test_true)\n",
    "        test_error = np.array(y_test_true)-np.array(y_test_pred)\n",
    "        test_result_dict['True'].extend(np.array(y_test_true))\n",
    "        test_result_dict['Prediction'].extend(np.array(y_test_pred))\n",
    "\n",
    "    return test_mae, test_acc, test_p,test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9306a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = list(ParameterGrid(param_cand))\n",
    "\n",
    "temp_param = param_grid[temp_sel_idx]\n",
    "\n",
    "h1 = temp_param[\"h1\"]\n",
    "h2 = temp_param[\"h2\"]\n",
    "\n",
    "dropout_h1 = temp_param[\"dropout_h1\"]\n",
    "dropout_h2 = temp_param[\"dropout_h2\"]\n",
    "\n",
    "pretrain = temp_param[\"pretrain\"]\n",
    "freeze_ext = temp_param[\"freeze_ext\"]\n",
    "freeze_prd = temp_param[\"freeze_prd\"]\n",
    "\n",
    "batch_size = temp_param[\"batch_size\"]\n",
    "learning_rate = temp_param[\"lr\"]\n",
    "epochs = temp_param[\"epochs\"]\n",
    "l2_param = temp_param[\"l2_param\"]\n",
    "\n",
    "lr_patience = temp_param[\"lr_patience\"]\n",
    "lr_factor = temp_param[\"lr_factor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_fin = pd.DataFrame()\n",
    "idx = 0\n",
    "test_error_dict = dict()\n",
    "test_result_dict = {'True':[],'Prediction':[]}\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "for n_outer_cv in select_fold+1:\n",
    "    print(n_outer_cv, end=' ')\n",
    "\n",
    "    outer_test_idx = outer_test_folds_idx[n_outer_cv-1]\n",
    "    X_test, y_test = X[outer_test_idx], y[outer_test_idx]\n",
    "    X_test = stats.zscore(X_test, axis=1)\n",
    "    outer_test_dataset = TestDataset(X_test, y_test)\n",
    "    outer_test_loader = DataLoader(\n",
    "        outer_test_dataset, batch_size=len(y_test), pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "    model = DNN(h1, h2, dropout_h1, dropout_h2, act_func_name).to(device)\n",
    "\n",
    "\n",
    "    fold_model_path = os.path.join(model_path,f\"Outer_fold_{n_outer_cv}/model_fold_{n_outer_cv}.pt\")\n",
    "    tmp_model = torch.load(fold_model_path)\n",
    "    current_state_dict = model.state_dict()\n",
    "    for key, value in tmp_model.items():\n",
    "        if key in current_state_dict.keys():\n",
    "            current_state_dict[key] = value\n",
    "    model.load_state_dict(current_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.L1Loss() \n",
    "    test_loss, test_acc, test_p,test_error = test(model, 1, outer_test_loader, criterion)\n",
    "    result_df.loc[n_outer_cv,'test_loss']=test_loss\n",
    "    result_df.loc[n_outer_cv,'test_acc']=test_acc\n",
    "    result_df.loc[n_outer_cv,'test_p']=test_p\n",
    "    test_error_dict[str(n_outer_cv)] = test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01007ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mean,loss_std = np.round(result_df['test_loss'].mean(),3),np.round(result_df['test_loss'].std(),3)\n",
    "corr_mean, corr_std = np.round(result_df['test_acc'].mean(),3),np.round(result_df['test_acc'].std(),3)\n",
    "\n",
    "print(f\"Loss {loss_mean} +- {loss_std}\")\n",
    "print(f\"Corr {corr_mean} +- {corr_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3141c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.to_csv(f\"/users/hjd/Revision/data/pre_training/pretraining_result_excluding_{str(exclude_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "304b166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f\"/users/hjd/Revision/data/pre_training/ABCD_rsfc_excluding_{str(exclude_pairs)}\",X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
